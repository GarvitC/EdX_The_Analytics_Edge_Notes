********************************** Unit-3: Logistic regression ************************************

1. Dependent variable is categorical, like binary 0 or 1, or generally only a limited number of 
    possible outcomes - Linear regression use will require rounding up the outcome to 0 or 1

2. Logistic regression is better for using with outcomes that are categorical. It predicts the probability 
    of the outcome variable being True, i.e. P(y = 1)

3. P(y = 0) = 1 - P(y = 1)

4. Logistic response function: P(y = 1) = 1/(1+exp(-(b0+b1*x1+b2*x2+....+bk*xk))) - Non-linear transformation
    of the Linear regression equation to produce number between 0 and 1
    
5. The response function takes value between 0 and 1, positive values are predictive of class 1 and negative
    ones are predictive of class 0
    
6. Odds = P(y = 1)/P(y = 0), Odds > 1 if y = 1 is more likely and Odds < 1 if y = 0 is more likely and 
    Odds = 1 for equally likely outcomes

7. Odds = exp(b0+b1*x1+b2*x2+....+bk*xk)
    "Logit" = log(Odds) = b0+b1*x1+b2*x2+....+bk*xk, the bigger the Logit, the bigger P(y = 1)

8. A positive coefficient value (b) increases the Logit which increases the P(y = 1), similarly a negative
    coefficient decreases the Logit thus decreasing P(y = 1)
    
9. Baseline method in logistic regression is to predict the most frequent outcome for all observations, 
    the percent of the most likely outcome gives the accuracy of this baseline model, 
    i.e. accuracy(baseline) = (num_obs with most frequent outcome)/(num_total outcomes)

10. Install a package in R - "install.packages(<package-name>)" then select the CRAN mirror of the 
    nearest location, e.g. install.packages("caTools")
    
11. Load the package by using the "library()" function, e.g. library(caTools) - need to do this for 
    every session

12. Split a sample - set a seed to initialize the random number generator by "set.seed()", then use 
    "sample.split(<outcome-variable>, SplitRatio=<percent in training set>)" to split the data randomly 
    and get the training and test sets, e.g. sample.split(data$col1, SplitRatio=0.75)
    
13. sample.split() randomly splits the data but ensures outcome is well balanced in both sets so that our
    test set is representative of our training set
    
14. sample.split() returns TRUE for observations to be included training set and FALSE for observations 
    to be included in the test set, use subset() to create the training and test set from the 
    resulting outcome of sample.split()
    
15. Generalized linear model - glm(DV ~ IV1 + IV2 + IV3 + ..., data=<train-data>, family=binomial)
    
16. summary(model) - similar to the linear regression output. AIC value corresponds to the adjusted 
    R squared. AIC value accounts for the number of variables used compared to the number of observations,
    can be compared between models on the same dataset. Preferred model is one with minimum AIC
    
17. "predict()" used to predict the outcome using the model built - predict(<model>, type = "response"),
    "type = response" tells the function to give probabilities
    
18. "summary(<prediction>)" gives statistics for the predictions (probabilities) like the mean value,
    min, max, Median etc. All the values are between 0 and 1 being the probabilities
    
19. tapply(<predictions>, data$outcome-variable, mean) - average value of the predictions for actual 
    TRUE/FALSE cases. Expected - Higher average value of probability for actual TRUE cases.
    
20. Outcome of the Logistic regression model is probability values, for getting to predict the 
    outcome(binary, generally), choose a threshold value (t), i.e. if P(<outcome-variable>=1) >= t
    predict 1, and if P(<outcome-variable>=1) < t predict 0
    
21. Select t on the basis of which prediction errors are better, a large value of t will make prediction
    of 1 rare, only when P(y=1) is quite large thus covering the extreme cases whereas a small value
    of t will predict 1 more frequently covering all cases where the actual value might be 1, pick
    t = 0.5 if there is no preference between the errors - predicts the more likely outcome
    
22. Quantitatively use the confusion matrix(classification matrix) to compare the True positives/negatives
    (TP/TN) and False positives/negatives (FP/FN) - Predicted outcomes are column labels, Actual outcomes
    are row labels
    
23. Sensitivity = TP/(TP+FN) - Percentage of actual 1's which are classified correctly, 
    called True positive rate
    Specificity = TN/(TN+FP) - Percentage of actual 0's which are classified correctly, 
    called True Negative rate
  
24. Model with higher threshold(t) - Lower Sensitivity and Higher Specificity 
    Model with lower threshold(t) - Higher Sensitivity and Lower Specificity 
    
25. Decide the threshold value by ROC (Receiver Operator Characteristic Curve):
    Y-axis - True positive rate (sensitivity)
    X-axis - False positive rate (1-specificity)
    (0,0) point corresponds to t=1, i.e. correctly identifying all the True negatives
    (1,1) point corresponds to t=0, i.e. correctly identifying all the True positives

26. All thresholds are captures by the ROC curve: 
    Closer to 0, higher t - Lower Sensitivity and Higher Specificity
    Closer to 1, lower t - Higher Sensitivity and Lower Specificity
    
27. Pick the threshold value based on the best trade off - cost of failing to detect positives
    OR the cost of raising false alarms.
    
28. ROC curve is plotted using the ROCR package's functions "prediction()" and "performance()"
    e.g. ROCRpred = prediction(<predictions>, <data$outcome-variable>)
        ROCRperf = performance(<output of prediction() i.e. ROCRpred>, "tpr", "fpr")
        plot(<output of performance() i.e. ROCRperf>, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), 
            text.adj=c(-0.2,1.7)) - last three arguments add the colour map, and labels for thresholds in 
                                    increments of 0.1 and position these labels respectively

29. Interpret the model - Check multicollinearity by looking at the coefficients (signs agree intuitively?)
                            and also by checking for any high correlation values
                        - Significance of the model by area under the ROC curve, perfect score would be an 
                            area of 1, minimum is 0.5 (pure guessing)
                   
30. For a confusion matrix with N observations total, outcome measures:
        - Overall Accuracy = (TP + TN)/N
        - Overall error rate = (FP + FN)/N 
        - Sensitivity(True Positive rate) = TP/(TP + FN)
        - Specificity(True Negative rate) = TN/(TN + FP)
        - False Negative error rate = FN/(TP + FN)
        - False Positive error rate = FP/(TN + FP)
        
31. For out of sample metrics the predict() function is used - predict(<model>, type="response", 
    newdata=<test-data>), find further metrics by using a threshold on the predicted probabilities and 
    using the confusion matrix 

32. For finding AUC(area under curve):
    predTest = predict(<model>, type="response", newdata=<test-data>)
    ROCRpredTest = prediction(predTest, <test-data$outcome-variable>)
    AUC = as.numeric(performance(ROCRpredTest, "auc")@y.values)

33. Impute missing values - install the package named "mice" (Multiple Imputation by Chained Equations), 
    then imputed = complete(mice(<dataset>)), initialize the seed value before by "set.seed(<number>)"
    
34. A smart baseline model is constructed in cases where the simple baseline model is too trivial
    [See the ELECTION PREDICTION example video]

35. "sign(num)" returns -1 is num<0, 0 if num=0 and 1 if num>0

36. Breakdown of the "sign" of a certain variable is taken from the training set to construct the smart 
    baseline model, e.g. table(sign(<data_train$col1>)) - Compare the baseline results by the command: 
    table(<data_train$outcome-variable>, sign(data_train$col1))
    